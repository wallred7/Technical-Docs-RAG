{"cells":[{"cell_type":"markdown","source":["## Introduction\n","\n","This notebook contains a script for retrieval augmented generation (RAG) using the Llama Index RAG framework [9], Milvus vector store [5], HuggingFace models [2,5], and TruLens evaluation [1]. The most prevalent approach to RAG leverages OpenAI's embeddings and inference model. This is apparent through the number of tutorials that use OpenAI in RAG systems [3,5,6,9,10]. However, there are a vast number of models available [7,8,9]. So there is value in exploring RAG performance of non-OpenAI alternatives. In this script, the embedding model was chosen from the HuggingFace leaderboard of embedding models [7]. The notebook experimented with the Gemma 2B [12] and Gemma 7B [13] due to its recent release, SoTA performance, and rising popularity."],"metadata":{"collapsed":false,"id":"2a78704317440b99"},"id":"2a78704317440b99"},{"cell_type":"code","outputs":[],"source":["# Clears the output of certain cells to maintain a cleaner notebook appearance\n","from IPython.display import clear_output"],"metadata":{"id":"4f6ed60bc70cbe0a","ExecuteTime":{"end_time":"2024-02-26T11:56:08.832488Z","start_time":"2024-02-26T11:56:08.818631Z"},"executionInfo":{"status":"ok","timestamp":1709204028351,"user_tz":0,"elapsed":8,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"4f6ed60bc70cbe0a","execution_count":1},{"cell_type":"code","execution_count":2,"id":"initial_id","metadata":{"collapsed":true,"id":"initial_id","executionInfo":{"status":"ok","timestamp":1709204075996,"user_tz":0,"elapsed":47652,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"outputs":[],"source":["# pip installations\n","!pip install llama-index\n","!pip install pymilvus\n","!pip install milvus\n","!pip install \"transformers[torch]\"\n","!pip install openai\n","clear_output(0) # output will clear after n wait time"]},{"cell_type":"code","source":["# colab % pip installations\n","%pip install llama-index-vector-stores-milvus\n","%pip install llama-index-embeddings-huggingface\n","%pip install llama-index-embeddings-openai\n","%pip install llama-index-llms-huggingface\n","clear_output()"],"metadata":{"id":"VNrakxL3LWZT","executionInfo":{"status":"ok","timestamp":1709204116719,"user_tz":0,"elapsed":40735,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"VNrakxL3LWZT","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Data, Embeddings, & Inference"],"metadata":{"id":"zmlOH0sngWUR"},"id":"zmlOH0sngWUR"},{"cell_type":"code","outputs":[],"source":["# imports\n","from llama_index.core import SimpleDirectoryReader, Settings\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.embeddings.openai import OpenAIEmbedding\n","from llama_index.llms.huggingface import HuggingFaceInferenceAPI"],"metadata":{"id":"1a177d20aecfa61c","ExecuteTime":{"end_time":"2024-02-26T11:57:01.980816Z","start_time":"2024-02-26T11:56:41.528597Z"},"executionInfo":{"status":"ok","timestamp":1709204125405,"user_tz":0,"elapsed":8696,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"1a177d20aecfa61c","execution_count":4},{"cell_type":"code","source":["# Read the Document\n","# links to a directory and reads in the relevant files.\n","# ./ points to the local dir.\n","docs = SimpleDirectoryReader('./').load_data()\n","\n","docs[0].extra_info"],"metadata":{"id":"o9lXN3_lAyJc","ExecuteTime":{"end_time":"2024-02-26T11:57:17.549211Z","start_time":"2024-02-26T11:57:04.825676Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709204128831,"user_tz":0,"elapsed":3442,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"outputId":"84033e0e-5c30-43f0-abdc-0d477821f29c"},"id":"o9lXN3_lAyJc","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'page_label': '1',\n"," 'file_name': '/content/MOOC-2 HumanSensing.pdf',\n"," 'file_path': '/content/MOOC-2 HumanSensing.pdf',\n"," 'file_type': 'application/pdf',\n"," 'file_size': 24406576,\n"," 'creation_date': '2024-02-29',\n"," 'last_modified_date': '2024-02-29',\n"," 'last_accessed_date': None}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# define function for embedding model to be used\n","\n","def define_embeddings_model(service: str, _model_name: str = None):\n","    \"\"\"\n","    Initializes and sets the embedding model based on the specified service.\n","\n","    Parameters:\n","    - service (str): The embedding service to use. Currently supports 'openai' or 'huggingface'.\n","    - model_name (str): The name or identifier of the embedding model (specific to the chosen service).\n","\n","    Raises:\n","    - ValueError: If the provided service is not 'openai' or 'huggingface'.\n","\n","    Returns:\n","    None\n","\n","    Example:\n","    >>> define_embeddings_model('openai', 'gpt-3.5-turbo', 'Hello, World!')\n","    Sets the embedding model to OpenAI's GPT-3.5-turbo for the input text 'Hello, World!'.\n","\n","    Notes: GPT-3.5-turbo authored this docstring, not the function.\n","    \"\"\"\n","    if service.lower() == 'openai':\n","        embed_model = OpenAIEmbedding()  # uses the default OpenAI Embedding model\n","    elif service.lower() == 'hf' or service.lower() == 'huggingface':\n","        embed_model = HuggingFaceInferenceAPI(model_name=_model_name)\n","    else:\n","        raise ValueError('Embedding service is not supported.')\n","\n","    Settings.embed_model = embed_model"],"metadata":{"id":"2cWx7fNICF6H","executionInfo":{"status":"ok","timestamp":1709207992402,"user_tz":0,"elapsed":4,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"2cWx7fNICF6H","execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["I experimented with two embedding models. 'Salesforce/SFR-Embedding-Mistral' [8] ranks highest overall and in retrieval on HuggingFace's Massive Text Embedding Benchmark (MTEB) [7]. 'WhereIsAI/UAE-Large-V1' [11] ranks highly in both categories but is much smaller allowing it to be run locally from a free Google Colab environment. The former requires use of the HuggingFace Inference API while the latter can be called via the HuggingFaceEmbedding function, as demonstrated below [4]."],"metadata":{"id":"SdmWZDpiwxpo"},"id":"SdmWZDpiwxpo"},{"cell_type":"code","source":["define_embeddings_model('hf','Salesforce/SFR-Embedding-Mistral')\n","# define_embeddings_model('hf','WhereIsAI/UAE-Large-V1')"],"metadata":{"id":"IlW3JU4Ht81_","executionInfo":{"status":"ok","timestamp":1709218689870,"user_tz":0,"elapsed":7,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"IlW3JU4Ht81_","execution_count":84,"outputs":[]},{"cell_type":"code","source":["# Use Open Source Emeddings\n","# This is an alternative method that references a local or downloadable embeddings model\n","model_name = 'WhereIsAI/UAE-Large-V1'\n","embed_model = HuggingFaceEmbedding(model_name)\n","\n","# Adds the selected embed model to be used by the Llama Index functions/framework\n","# This functionality replaces service_context in Llama Index recent v10 release\n","Settings.embed_model = embed_model"],"metadata":{"id":"hErswjljESDw","executionInfo":{"status":"ok","timestamp":1709218801210,"user_tz":0,"elapsed":2671,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"hErswjljESDw","execution_count":93,"outputs":[]},{"cell_type":"code","source":["# This prints examples of the embeddings to make sure it's working properly\n","embeddings = embed_model.get_text_embedding(\"Hello World!\")\n","print(len(embeddings))\n","print(embeddings[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pBxbfEkQL0Oc","executionInfo":{"status":"ok","timestamp":1709218802872,"user_tz":0,"elapsed":421,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"outputId":"481e953f-188d-42cd-a111-a2e060d5c990"},"id":"pBxbfEkQL0Oc","execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["1024\n","[0.005473877303302288, 0.056089356541633606, 0.015606299974024296, -0.014848134480416775, -0.03771238774061203]\n"]}]},{"cell_type":"code","source":["# selection of the inference model\n","inf_model = 'google/gemma-7b'\n","# inf_model = 'google/gemma-2b'\n","\n","llm = HuggingFaceInferenceAPI(model_name=inf_model)\n","Settings.llm = llm"],"metadata":{"id":"SLYs96-eJZgU","executionInfo":{"status":"ok","timestamp":1709218689870,"user_tz":0,"elapsed":6,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"SLYs96-eJZgU","execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["## Vector Database"],"metadata":{"id":"gUx3evikgnrr"},"id":"gUx3evikgnrr"},{"cell_type":"code","source":["from llama_index.core import VectorStoreIndex, Document, StorageContext\n","from llama_index.vector_stores.milvus import MilvusVectorStore\n","from milvus import default_server, debug_server\n","from pymilvus import connections, utility"],"metadata":{"id":"r63ql4jHg976","executionInfo":{"status":"ok","timestamp":1709211429611,"user_tz":0,"elapsed":6,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"r63ql4jHg976","execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["Milvus proved to be temperamental. The primary service is incompatible with a Jupyter Notebook and Google Colab environment [3]. As a solution, Milvus Lite was created [5]. However, it will only run on the initial run of the script. Any subsequent runs will cause the index to be unable to connect to the Milvus server/vector store. This makes the package setup disorienting. It also requires users to reload the colab environment entirely after each working session."],"metadata":{"id":"5llKJYZ-zKtU"},"id":"5llKJYZ-zKtU"},{"cell_type":"code","source":["# starts a milvus server for the vector database to use\n","# stops any currently running servers\n","default_server.stop()\n","\n","try:\n","  default_server.cleanup() # cleans up previous data\n","except:\n","  print('Server is not running.')\n","\n","default_server.start()\n","\n","connections.connect(host='127.0.0.1', port=default_server.listen_port)\n","\n","print(utility.get_server_version())\n","print(default_server.listen_port)"],"metadata":{"id":"Q8XOe7nqWmcO","executionInfo":{"status":"ok","timestamp":1709204168818,"user_tz":0,"elapsed":11434,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"846f280c-065f-4c8c-8881-01ccdb2b3b0e"},"id":"Q8XOe7nqWmcO","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["v2.3.5-lite\n","19530\n"]}]},{"cell_type":"code","source":["# Create an Index\n","vector_store = MilvusVectorStore(dim=1024, overwrite=True)\n","storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)"],"metadata":{"id":"eeX07aSGGhYc","ExecuteTime":{"end_time":"2024-02-26T11:57:35.813801Z","start_time":"2024-02-26T11:57:20.253518Z"},"executionInfo":{"status":"ok","timestamp":1709219249454,"user_tz":0,"elapsed":435571,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"59997e28-cf65-412f-e1ee-819063da7ae5"},"id":"eeX07aSGGhYc","execution_count":95,"outputs":[{"output_type":"stream","name":"stderr","text":["DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 4dd7a568c16f46528dae2e0257249d22\n","DEBUG:pymilvus.milvus_client.milvus_client:Successfully created collection: llamacollection\n","DEBUG:pymilvus.milvus_client.milvus_client:Successfully created an index on collection: llamacollection\n"]}]},{"cell_type":"code","source":["# Query Engine\n","query_engine = index.as_query_engine()"],"metadata":{"id":"QQvgq9DtnSRi","executionInfo":{"status":"ok","timestamp":1709219249454,"user_tz":0,"elapsed":21,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"QQvgq9DtnSRi","execution_count":96,"outputs":[]},{"cell_type":"code","source":["# Test Query\n","res = query_engine.query('What is a common form of human sensing?')\n","res"],"metadata":{"id":"MqajI8NWozlX"},"id":"MqajI8NWozlX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TruLens Evaluation"],"metadata":{"id":"FP8wL6nbgQD4"},"id":"FP8wL6nbgQD4"},{"cell_type":"markdown","source":["TruLens evaluations frameworks provides many useful tools for assessing the performance of RAG systems. It keeps a record of experiments, although it does not record different backend inference or embedding models. It provides functions to create metrics such as language match, groundedness, question answer relevance, and question search relevance. It provides a Streamlit-based dashboard for viewing and analysing the results of different experiments [1]."],"metadata":{"id":"GN6prmzN32JI"},"id":"GN6prmzN32JI"},{"cell_type":"code","source":["!pip install trulens_eval\n","clear_output()"],"metadata":{"id":"VIL0XkSYhrP2","executionInfo":{"status":"ok","timestamp":1709204622255,"user_tz":0,"elapsed":26529,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"VIL0XkSYhrP2","execution_count":15,"outputs":[]},{"cell_type":"code","source":["# TruLens\n","from trulens_eval import Tru\n","tru = Tru()\n","clear_output()"],"metadata":{"id":"eJD3wMnXgMws","executionInfo":{"status":"ok","timestamp":1709218701700,"user_tz":0,"elapsed":408,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"eJD3wMnXgMws","execution_count":87,"outputs":[]},{"cell_type":"code","source":["from trulens_eval.app import App\n","from trulens_eval import Feedback\n","from trulens_eval.feedback import Groundedness\n","from trulens_eval.feedback.provider.hugs import Huggingface\n","from trulens_eval.feedback.provider.openai import OpenAI\n","import numpy as np\n","\n","# define context used in feedback\n","context = App.select_context(query_engine)\n","\n","# provides the model on which the following metrics are calculated\n","provider = OpenAI()\n","\n","# alternative option for evaluation metrics\n","# although HF doesn't support the same metrics\n","# provider = Huggingface()\n","\n","# groundedness\n","grounded = Groundedness(groundedness_provider=provider)\n","f_groundedness = (\n","    Feedback(grounded.groundedness_measure_with_cot_reasons)\n","    .on(context.collect()) # collect context chunks into a list\n","    .on_output()\n","    .aggregate(grounded.grounded_statements_aggregator)\n",")\n","\n","# question and answer relevance\n","f_qa_relevance = Feedback(provider.relevance).on_input_output()\n","\n","# question search relevance\n","f_qs_relevance = (\n","    Feedback(provider.qs_relevance)\n","    .on_input()\n","    .on(context)\n","    .aggregate(np.mean)\n",")"],"metadata":{"id":"4XdX2mYPrx-1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709219250130,"user_tz":0,"elapsed":681,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"outputId":"c89a2975-cb85-4677-ea0f-c35c71045c90"},"id":"4XdX2mYPrx-1","execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n","‚úÖ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n","‚úÖ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n","‚úÖ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n","‚úÖ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n","‚úÖ In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"]}]},{"cell_type":"code","source":["# TruLens Recording/Logging\n","from trulens_eval import TruLlama\n","query_engine_recorder = TruLlama(query_engine, feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance])"],"metadata":{"id":"NK0NwshWMSCv","executionInfo":{"status":"ok","timestamp":1709219252257,"user_tz":0,"elapsed":2138,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"NK0NwshWMSCv","execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["## Prompt Experimentation\n","\n","I choose to experiment with different retrieval questions to qualitatively assess the RAG system's capacity to return coherent and accurate information. On a variety of prompts, the system returned structured accurate information for the large data source. The output was prone to repetition. The model has not undergone instruction fine-tuning. Despite this, I tried issuing an instruction to 'avoid repeating', which had little effect. The best prompt was the final uncommented string that asked about sensing sleep. This returned a numbered list of relevant devices. This seems to indicate that the system benefits from additional specificity."],"metadata":{"id":"73Q4EzwT3woh"},"id":"73Q4EzwT3woh"},{"cell_type":"code","source":["\n","# prompt = 'Who is the author of presentation?'\n","# prompt = 'After sensory data is gathered, what are some examples of its usage?'\n","# prompt = 'What kinds of sensors are there?'\n","# prompt = 'Avoid repeating, what can sensory data be used for?'\n","prompt = 'What devices are used for sensing sleep?'\n","\n","with query_engine_recorder as recording:\n","    query_engine.query(prompt)"],"metadata":{"id":"EhiMwzvxOP0O","executionInfo":{"status":"ok","timestamp":1709222432815,"user_tz":0,"elapsed":4421,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"EhiMwzvxOP0O","execution_count":115,"outputs":[]},{"cell_type":"code","source":["# display recordings\n","rec = recording.get()\n","\n","display(rec)"],"metadata":{"id":"S5af9nsfKlqO"},"id":"S5af9nsfKlqO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# view results of the feedback function\n","for feedback, feedback_result in rec.wait_for_feedback_results().items():\n","    print(feedback.name, feedback_result.result)"],"metadata":{"id":"gSzfyEKbWXlt"},"id":"gSzfyEKbWXlt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tru.get_leaderboard()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"7CY_3F5SSeKW","executionInfo":{"status":"ok","timestamp":1709220976604,"user_tz":0,"elapsed":378,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"outputId":"af7495dd-3d20-4725-b922-ab5caeec5fa7"},"id":"7CY_3F5SSeKW","execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                           latency  total_cost\n","app_id                                                        \n","Example1                                       6.0         0.0\n","app_hash_0ebeb74bd1fb0fcb293283b1389190c4      6.0         0.0\n","app_hash_0f197a1def38758ccfb392dea7b14c6b      4.0         0.0\n","app_hash_a28c53e34ac14e64895744a94bdd813c      6.0         0.0\n","app_hash_ad067d64c00c21b37c5a3597f54433c0      4.0         0.0\n","app_hash_df319f4f4b118bd79c19640e56cacfc5      6.0         0.0"],"text/html":["\n","  <div id=\"df-4a04a30e-0af9-42be-8b3e-8d10e68f3fd3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>latency</th>\n","      <th>total_cost</th>\n","    </tr>\n","    <tr>\n","      <th>app_id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Example1</th>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>app_hash_0ebeb74bd1fb0fcb293283b1389190c4</th>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>app_hash_0f197a1def38758ccfb392dea7b14c6b</th>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>app_hash_a28c53e34ac14e64895744a94bdd813c</th>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>app_hash_ad067d64c00c21b37c5a3597f54433c0</th>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>app_hash_df319f4f4b118bd79c19640e56cacfc5</th>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a04a30e-0af9-42be-8b3e-8d10e68f3fd3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4a04a30e-0af9-42be-8b3e-8d10e68f3fd3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4a04a30e-0af9-42be-8b3e-8d10e68f3fd3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f5f01914-945c-4593-9200-8d86f4e58db7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5f01914-945c-4593-9200-8d86f4e58db7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f5f01914-945c-4593-9200-8d86f4e58db7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"tru\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0327955589886444,\n        \"min\": 4.0,\n        \"max\": 6.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4.0,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["tru.run_dashboard()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2CBiGsMOgWh","executionInfo":{"status":"ok","timestamp":1709220696698,"user_tz":0,"elapsed":10351,"user":{"displayName":"William Allred","userId":"07616141287706591162"}},"outputId":"5ec9f752-436d-4786-f88e-70da9b20751f"},"id":"e2CBiGsMOgWh","execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting dashboard ...\n","Config file already exists. Skipping writing process.\n","Credentials file already exists. Skipping writing process.\n","npx: installed 22 in 9.24s\n","\n","Go to this url and submit the ip given here. your url is: https://salty-geese-train.loca.lt\n","\n","  Submit this IP Address: 35.236.183.2\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["tru.stop_dashboard()"],"metadata":{"id":"C2cAz-XppVSd","executionInfo":{"status":"ok","timestamp":1709222634905,"user_tz":0,"elapsed":353,"user":{"displayName":"William Allred","userId":"07616141287706591162"}}},"id":"C2cAz-XppVSd","execution_count":116,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion\n","\n","The system provides accurate question and answering functionality despite using a smaller non-OpenAI LLM as a backbone. This supports the view that non-massive models still have their place in the modern machine learning ecosystem."],"metadata":{"id":"DgrDX_Mdju34"},"id":"DgrDX_Mdju34"},{"cell_type":"markdown","source":["## Suggested Improvements\n","\n","This project was the perfect level of challenging. Llama Index provided extensive documentation that was essential. I recommend opening the vector store to the student's discretion. There are many companies creating their own vector databases and it would be a helpful exercise to review popular technologies and make a selection. I was unimpressed by Milvus so, if you would rather not have students make an open selection, I would change the recommended service from Milvus to Chroma DB.\n","\n","Next, I would encourage use of non-OpenAI services. There is little doubt that OpenAI is the industry standard and their models are commonly the benchmarks with which new models are compared. However, it is beneficial to go through the exercise of embedding and inference model selection because it teaches students the resources they need to make those decisions. There are many situations where smaller models are a better choice than the extremely large GPT models.\n"],"metadata":{"id":"93Vq-_S-765A"},"id":"93Vq-_S-765A"},{"cell_type":"markdown","source":["## Citations\n","\n","[1] ‚Äúü§ó HuggingFace - ü¶ë TruLens.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://www.trulens.org/trulens_eval/api/provider/huggingface/#trulens_eval.feedback.provider.hugs.Huggingface.pii_detection\n","\n","[2] ‚ÄúBuilding RAG from Scratch (Open-source only!) - LlamaIndex ü¶ô v0.10.14.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval.html\n","\n","[3] ‚ÄúGet Started with Milvus Lite.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://milvus.io/docs/milvus_lite.md\n","\n","[4] ‚ÄúLocal Embeddings with HuggingFace - LlamaIndex ü¶ô v0.10.14.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface.html\n","\n","[5] ‚ÄúMilvus Vector Store - LlamaIndex ü¶ô v0.10.14.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo.html\n","\n","[6] ‚Äúmilvus-lite/examples/example.py at main ¬∑ milvus-io/milvus-lite ¬∑ GitHub.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://github.com/milvus-io/milvus-lite/blob/main/examples/example.py\n","\n","[7] ‚ÄúMTEB Leaderboard - a Hugging Face Space by mteb.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://huggingface.co/spaces/mteb/leaderboard\n","\n","[8] ‚ÄúSalesforce/SFR-Embedding-Mistral ¬∑ Hugging Face.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://huggingface.co/Salesforce/SFR-Embedding-Mistral\n","\n","[9] ‚ÄúStarter Tutorial - LlamaIndex ü¶ô v0.10.14.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\n","\n","[10] ‚ÄúUsing LLMs - LlamaIndex ü¶ô v0.10.14.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms.html\n","\n","[11] ‚ÄúWhereIsAI/UAE-Large-V1 ¬∑ Hugging Face.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://huggingface.co/WhereIsAI/UAE-Large-V1\n","\n","[12] ‚Äúgoogle/gemma-2b ¬∑ Hugging Face.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://huggingface.co/google/gemma-2b\n","\n","[13] ‚Äúgoogle/gemma-7b ¬∑ Hugging Face.‚Äù Accessed: Feb. 29, 2024. [Online]. Available: https://huggingface.co/google/gemma-7b\n","\n"],"metadata":{"collapsed":false,"id":"28ed197b8791c7d8"},"id":"28ed197b8791c7d8"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
